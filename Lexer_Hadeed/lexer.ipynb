{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902694d0",
   "metadata": {},
   "source": [
    "# **Lexer Construction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508316f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from token_definition import token_defs\n",
    "from tokenize_regex import tokenize_regex\n",
    "from postfix_conversion import to_postfix\n",
    "from postfix_to_nfa import postfix_to_nfa,combine_nfas,print_nfa\n",
    "from nfa_to_dfa import nfa_to_dfa\n",
    "from plot_nfa import plot_nfa_with_graphviz\n",
    "from optimize_dfa import DFAOptimizer\n",
    "from dfa_table import DFATable\n",
    "from scanner import LexicalScanner\n",
    "\n",
    "\n",
    "print(\"Processing Lexer Token Definitions...\")\n",
    "print(\"=\"*80)\n",
    "    \n",
    "results = []\n",
    "for token_name, regex in token_defs:\n",
    "    print(f\"\\nüìù Processing: {token_name} -> {regex}\")\n",
    "        \n",
    "    # Tokenize and convert to postfix\n",
    "    tokens = tokenize_regex(regex)\n",
    "    print(f\"   Tokens: {tokens}\")\n",
    "        \n",
    "    postfix = to_postfix(tokens)\n",
    "    print(f\"   Postfix: {postfix}\")\n",
    "    \n",
    "    # Convert to NFA and display\n",
    "    nfa = postfix_to_nfa(postfix)\n",
    "    if nfa:\n",
    "        # print_nfa(nfa)\n",
    "        filename = f\"lexer_{token_name.lower()}\"\n",
    "        display_result = plot_nfa_with_graphviz(nfa, f\"{token_name}: {regex}\")\n",
    "        results.append((token_name, regex, nfa, None))\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to create NFA for {token_name}\")\n",
    "        \n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "#Combine all NFAs into a single NFA and display\n",
    "\n",
    "final_nfa = combine_nfas(results)\n",
    "# if final_nfa:\n",
    "    # print(\"\\n Combined Final NFA:\")\n",
    "    # print_nfa(final_nfa)\n",
    "    # plot_nfa_with_graphviz(final_nfa, \"Combined Final NFA\", \"final_combined_nfa\")\n",
    "    \n",
    "#Convert the final NFA to a DFA\n",
    "print(\"\\nüîÑ Converting NFA to DFA...\")\n",
    "start_dfa,all_dfa_states = nfa_to_dfa(final_nfa)\n",
    "                \n",
    "# print(\"\\n‚úÖ DFA States and Transitions:\")\n",
    "# for state in all_dfa_states:\n",
    "#     acc = \"Accepting\" if state.is_accepting else \"Non-Accepting\"\n",
    "#     token = f\"Token: {state.token_type}\" if state.token_type else \"\"\n",
    "#     print(f\"{state} ({acc}) {token}\")\n",
    "#     for symbol, target in state.transitions.items():\n",
    "#         print(f\"   {state} --{symbol}--> {target}\")\n",
    "        \n",
    "#Optimize DFA\n",
    "optimizer = DFAOptimizer(start_dfa, all_dfa_states)\n",
    "min_start, min_states = optimizer.minimize_dfa()\n",
    "\n",
    "# Build transition table\n",
    "dfa_table = DFATable(min_start, min_states)\n",
    "dfa_table.print_table_stats()\n",
    "dfa_table.print_table()\n",
    "dfa_table.export_to_excel() \n",
    "    \n",
    "# Create scanner\n",
    "scanner = LexicalScanner(dfa_table, skip_terminators=True)    \n",
    "print(\"‚úÖ Scanner ready for use!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e405061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test the scanner with sample code\"\"\"\n",
    "test_code = '''integer main () {\n",
    "    integer x equals to 5 semicolon\n",
    "    integer y equals to 10 semicolon\n",
    "    boolean result equals to x is less than y semicolon\n",
    "    if (result) {\n",
    "        integer sum equals to x add y semicolon\n",
    "        }\n",
    "}'''\n",
    "    \n",
    "print(f\"\\nüß™ Testing scanner with sample code:\")\n",
    "print(f\"Input: {repr(test_code)}\")\n",
    "print(f\"Length: {len(test_code)} characters\")\n",
    "    \n",
    "tokens = scanner.scan(test_code)\n",
    "    \n",
    "print(f\"\\nüìù Generated tokens:\")\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"  {i+1:2d}. {token}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0124b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Function with a loop, float arithmetic, and return\n",
    "test_code_loop = '''\n",
    "float compute_area ( float radius ) {\n",
    "    float pi equals to 3.14 semicolon\n",
    "    float area equals to pi multiply radius multiply radius semicolon\n",
    "    return area semicolon\n",
    "}\n",
    "'''\n",
    "\n",
    "print(f\"\\nüß™ Testing scanner with sample code:\")\n",
    "print(f\"Input: {repr(test_code_loop)}\")\n",
    "print(f\"Length: {len(test_code_loop)} characters\")\n",
    "    \n",
    "tokens = scanner.scan(test_code_loop)\n",
    "    \n",
    "print(f\"\\nüìù Generated tokens:\")\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"  {i+1:2d}. {token}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b00d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Conditional with relational and logical ops, booleans, and strings\n",
    "test_code_cond = '''\n",
    "boolean check_user ( string name , integer age ) {\n",
    "    boolean is_adult equals to age is greater than or equal to 18 semicolon\n",
    "    boolean valid_name equals to name dot length is greater than 0 semicolon\n",
    "    if ( is_adult and valid_name ) {\n",
    "        string msg equals to \"Access granted\" semicolon\n",
    "    } else {\n",
    "        string msg equals to \"Access denied\" semicolon\n",
    "    }\n",
    "    return is_adult semicolon\n",
    "}\n",
    "'''\n",
    "\n",
    "print(f\"\\nüß™ Testing scanner with sample code:\")\n",
    "print(f\"Input: {repr(test_code_cond)}\")\n",
    "print(f\"Length: {len(test_code_cond)} characters\")\n",
    "    \n",
    "tokens = scanner.scan(test_code_cond)\n",
    "    \n",
    "print(f\"\\nüìù Generated tokens:\")\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"  {i+1:2d}. {token}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a972edf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
